---
title: "An Overview of the MFA package"
author:
output: 
  rmarkdown::html_vignette:
    toc: true # table of content true
    depth: 4  # upto three depths of headings (specified by #, ## and ###)
vignette: >
  %\VignetteIndexEntry{MFA}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---
\fontsize{12}{12}
```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
options(tibble.print_min = 4L, tibble.print_max = 4L,width = 100)
library(MFA)
```
This vignette gives an introduction to the model Multiple Factor Analysis and an overview the package MFA developed to apply the model on data. You might find reading this entire vignette helpful to get a broad understanding of what can be done in R using the MFA. 

## 1 Introdution to Multiple Factor Analysis {#sec:line-type-spec}

### 1.1 Overview

Multiple factor analysis (MFA, also called multiple factorial analysis) is an generalization of principal component analysis (PCA). Its goal is to analyze several data sets of variables collected on the same set of observations, or—as in its dual version—several sets of observations measured on the same set of variables. 

The goals of MFA are (1) to analyze several data sets measured on the same observations; (2) to provide a set of common factor scores (often called ‘compromise factor scores’); and (3) to project each of the original data sets onto the compromise to analyze communalities and discrepancies. 

### 1.2 When to use

MFA is used when several sets of variables have been measured on the same set of observations. The number and/or nature of the variables used to describe the observations can vary from one set of variables to the other, but the observations should be the same in all the data sets.

For example, suppose we have 12 wines evaluated by 10 different wine experts. The different data sets can have the same observations (wines) evaluated by different subjects (wine experts) or groups of sub- jects with different variables (each wine expert evaluates the wines with his/her own set of scales). In this case, the first data set corresponds to the first subject, the second one to the second subject and so on. The goal of the analysis, then, is to evaluate if there is an agreement between the subjects or groups of subjects.


### 1.3 Main Idea 

The general idea behind MFA is to normalize each of the individual data sets so that their first principal component has the same length. There are several terms with regards to MFA: 

* **compromise/consensus**: Obtained by combining normalized individual data table into a common representation of the observations.

* **factor scores**: The coordinates of the observations on the components. These can be used to plot maps of the ob- servations. 

* **partial factor scores**: The positions of the observations ‘as seen by’ each data set. These can be also represented as points in the compromise map. 

* **loading**: The quantity that variables contributes a certain amount to each component. This reflects the importance of that variable for this component and can also be used to plot maps of the variables that reflect their association. 

* **contributions**: A variation over squared loadings. These evaluate the importance of each variable as the pro- portion of the explained variance of the component by the variable. The contribution of a data table to a component can be obtained by adding the contributions of its variables. These contributions can then be used to draw plots expressing the importance of the data tables in the common solution.

## 2 The mathematics behind the package 

This section elaborates mathematical methods related to the model that are used in the development of MFA package.

### SVD

Recall that the $SVD$ of a given $I × J$ matrix $\textbf{Z}$ decomposes it into three matrices as:
$$\begin{equation}
\textbf{X} = \textbf{U} \Gamma \textbf{V}^T\quad \text{with} \quad  \textbf{U}^T\textbf{U} = \textbf{V}^T\textbf{V} = \textbf{I}.
\end{equation}$$
This is closely related to and generalizes the well-known $eigendecomposition$ as $\textbf{U}$ is also the matrix of the normalized eigenvectors of $\textbf{X} \textbf{X}^T$, $\textbf{V}$ is the matrix of the normalized eigenvectors of $\textbf{X}^T \textbf{X}$.
Notice that  $\textbf{X} \textbf{X}^T$ is denoted as $\textbf{S}$, which called cross product matrix of $\textbf{X}$. Therefore, if we do $eigendecomposition$ for $\textbf{S}$, we can get eigenvalue $\boldsymbol{\Lambda}$(i.e. $\Gamma^2$) and eigenvector $\textbf{U}$ (i.e. the left singular vector of $\textbf{X}$)  

*Key property*: the $SVD$ provides the best reconstitution (in a least squares sense) of the original matrix by a matrix with a lower rank.

### GSVD

The $GSVD$ generalizes the $SVD$ of a matrix by incorporating two additional positive definite matrices that represent ‘constraints’ to be incorporated in the decomposition. We call these two matrices *Mass Matrix* $\textbf{M}$ representing the ‘constraints’ imposed on the rows of an $I$ by $J$ matrix $\textbf{X}$ and *Weight Matrix* $\textbf{A}$ representing the ‘constraints’ imposed on the columns of $\textbf{X}$. Matrix $\textbf{M}$ is almost always a diagonal matrix of the ‘masses’ of the observations (i.e., the rows); whereas matrix $\textbf{A}$ implements a metric on the variables and if often but not always diagonal.Obviously, when $\textbf{M} = \textbf{A} = \textbf{I}$, the $GSVD$ reduces to the plain $SVD$.The $GSVD$ of $\textbf{X}$, taking into account $\textbf{M}$ and $\textbf{A}$, is expressed as 
$$\begin{equation}
\textbf{X}=\textbf{P}\boldsymbol{\Delta}\textbf{Q}^T \quad \text{with} \quad \textbf{P}^T\textbf{M}\textbf{P} = \textbf{Q}^T\textbf{A}\textbf{Q} = \textbf{I}.
\end{equation}$$
where $\textbf{P}$ is the $I$ by $L$ matrix of the normalized left generalized singular vectors (with $L$ being the rank of $\textbf{X}$), $\textbf{Q}$ the $J$ by $L$ matrix of the normalized generalized right singular vectors, and $\boldsymbol{\Delta}$ the $L$ by $L$ diagonal matrix of the $L$ generalized singular values.  

*Key property*: the $GSVD$ provides the best reconstitution (in a least squares sense) of the original matrix by a matrix with a lower rank under the constraints imposed by two positive definite matrices. The generalized singular vectors are orthonormal with respect to their respective matrix of constraints.  


### 2.1 DATA Preprocesing

* Centering and Scaling：It's always desirable to preprocess data before any data analysis process by first centering and normalizing each column such that its mean is equal to 0 and the sum of the square values of all its elements is equal to 1. 

* Concatenating data tables：The raw data consist of $K$ data sets collected on the same observations. Each data set is also called a table, a sub-table, or a block. The data for each table are stored in an $I × J_{[k]}$ rectangular data matrix denoted by $\textbf{Y}_{[k]}$, where $I$ is the number of observations and $J_{[k]}$ the number of variables collected on the observations for the $k$-th table. The total number of variables is denoted by $J$ (i.e., $J = \sum  J_{[k]}$). Then use `scale` function to normalize the data. 
The $K$ data matrices $\textbf{X}_{[k]}$ are concatenated into the complete $I$ by $J$ data matrix denoted by $\textbf{X}$:
$$\begin{equation}
\textbf{X}=[\textbf{X}_{[1]}|...|\textbf{X}_{[k]}|...|\textbf{X}_{[K]}].
\end{equation}$$

### 2.2 Mass Matrix

A mass, denoted by $m_i$, is assigned to each observation. These masses are collected in the mass vector, denoted by $\textbf{m}$, and in the diagonal elements of the mass matrix denoted by $\textbf{M}$, which is obtained as 
$$\begin{equation}
\textbf{M}=\text{diag}\{\textbf{m}\}
\end{equation}$$


### 2.3 Weight Matrix 

The weight matrix gathered by doing standard $PCA$ of each Data Table. Specifically, each table is expressed via its $SVD$ as
$$\begin{equation}
\textbf{X}_{[k]} = \textbf{U}_{[k]} \Gamma_{[k]}\textbf{V}^T_{[k]}\quad \text{with} \quad  \textbf{U}^T_{[k]}\textbf{U}_{[k]} = \textbf{V}^T_{[k]}\textbf{V}_{[k]} = \textbf{I}.
\end{equation}$$
In MFA, the weight of a table is obtained from the first singular value of its $PCA$. This weight, denoted by $\alpha_k$, is equal to the inverse of the first squared singular value:
$$\begin{equation}
\alpha_k=\frac{1}{\gamma_{1,k}^2}=\gamma_{1,k}^{-2}.
\end{equation}$$
For convenience, gather $\alpha$ weights in a $J$ by $1$ vector. Specifically, $\textbf{a}$ is constructed as:
$$\begin{equation}
\textbf{a}=[\alpha_1\textbf{1}_{[1]}^T,...,\alpha_k\textbf{1}_{[k]}^T,...,\alpha_K\textbf{1}_{[K]}^T],
\end{equation}$$
where $\alpha_k$ stands for the inverse of the first squared singular value of $k$-th block and $\textbf{1}_{[k]}$ for a $J_{[k]}$ vector of ones. Alternatively, the weights can be stored as the diagonal elements of a diagonal matrix denoted by $\textbf{A}$ obtained as
$$\begin{equation}
\textbf{A}=\textbf{diag}\{\textbf{a}\}.
\end{equation}$$

### 2.4 Calculating Cross-Product Matrices

After the weights have been collected, they are used to compute the $GSVD$ of $\textbf{X}$ under the constraints provided by $\textbf{M}$ and $\textbf{A}$.
In this $MFA$ package, we choose to use the cross-product matrices $\textbf{S}_{[k]}$ to compute all elements of $MFA$. The cross-product matrices can be directly computed from $\textbf{X}$ as 
$$\begin{equation}
\textbf{S}_{[+]}=\textbf{X}\textbf{A}\textbf{X}^{\textbf{T}}.
\end{equation}$$

### 2.5 Obtain eiganvalues and loadings{#anchor}

Once we got the cross product matrices, we can do *eigendecomposition* to get eigenvector and eigenvalue of $\textbf{S}_{[+]}$, denote as $\textbf{U}$ and $\boldsymbol{\lambda}$ respectively. Actually,
$$\begin{equation}
\textbf{S}_{[+]}=\textbf{U}\boldsymbol{\lambda}\textbf{U}^T=\textbf{P}\sqrt{\textbf{M}}\boldsymbol{\lambda} \sqrt{\textbf{M}}^T\textbf{P}^T=\textbf{P}\boldsymbol{\Lambda}\textbf{P}^T
\end{equation}$$

Thus, the generalized *eigendecomposition* under the constraints provided by matrix $\textbf{M}$ of the compromise gives: 
$$\begin{equation}
\textbf{S}_{[+]}=\textbf{P}\Lambda \textbf{P}^{\textbf{T}} \quad \text{with} \quad \textbf{P}^{\textbf{T}} \textbf{M}\textbf{P}=\textbf{I}.
\end{equation}$$
Then we can compute eigenvalues $\boldsymbol{\Lambda}$, left generalized singular vectors $\textbf{P}$ and right generalized singular vectors $\textbf{Q}$ given by:

$$\begin{equation}
\boldsymbol{\Lambda}=\sqrt{\textbf{M}}\boldsymbol{\lambda} \sqrt{\textbf{M}}^T
\end{equation}$$
$$\begin{equation}
\textbf{P}=\textbf{U}\sqrt{\textbf{M}}^{-1}
\end{equation}$$

$$\begin{equation}
\textbf{Q}=\textbf{X}^\textbf{T}\textbf{M}\textbf{P}\sqrt{\boldsymbol{\Lambda}}^{-1}
\end{equation}$$
where $\boldsymbol{\lambda}$ and $\textbf{U}$ is the eigenvalue and eigenvector calculated by `eigen` function on $\textbf{S}_{[+]}$. 


### 2.6 Compromise Factor Scores and Partial Factor Scores

Once we got eigenvalue $\boldsymbol{\Lambda}$, left generalized singular vectors $\textbf{P}$ and right generalized singular vectors $\textbf{Q}$, we can compute partial factor score $\textbf{F}_{[k]}$ and common factor score $\textbf{F}$ obtained from:
$$\begin{equation}
\textbf{F}_{[k]}=K\alpha_k\textbf{X}_{[k]}\textbf{Q}_{[k]}=K\alpha_k\textbf{X}_{[k]}\textbf{X}_{[k]}^T\textbf{M}\textbf{P}\boldsymbol{\Delta}^{-1}
\end{equation}$$
$$\begin{equation}
\textbf{F}=\frac{1}{K}\sum_k \textbf{F}_{[k]}
\end{equation}$$

### 2.7 Contributions

In this part, we calculate three kind of contributions: contribution of an observation to a dimension,contributions of a variable to a dimension and contribution of a table to a dimension.

#### 2.7.1 Contributions of Observation{#anchor1}

Formally, the contribution of observation $i$ to component $l$, denoted $ctr_{i,l}$, is computed as 
$$\begin{equation}
ctr_{i,l}=\frac{m_i × f_{i,l}^2}{\lambda_l} \quad \text{with} \quad \lambda_l=\sum_i m_i × f_{i,l}^2
\end{equation}$$
where $m_i$ and $f_{i,l}$ are, respectively, the mass of the $i$th observation and the factor score of the $i$th observation for the $l$th dimension.   

#### 2.7.2 Contributions of Variables{#anchor2}

As we did for the observations, we can find the important variables for a given dimension by computing variable contributions.The contribution of variable $j$ to component $l$, denoted $ctr_{j,l}$, is computed as
$$\begin{equation}
ctr_{j,l}=a_j × q_{j,l}^2 \quad \text{with} \quad 1=\sum_j a_j × q_{j,l}^2
\end{equation}$$
where $q_{i,l}$ is the loading of the $j$th variable for the $l$th dimension.  

#### 2.7.3 Contributions of Table{#anchor3}

As a table comprises several variables, the contribution of a table can simply be defined as the sum of the contributions of its variables. So the contribution of table k to component l is denoted $ctr_{k,l}$ and is defined as 
$$\begin{equation}
ctr_{k,l}=\sum_j^{J_{[k]}} ctr_{j,l}
\end{equation}$$
Contributions take values between 0 and 1, and for a given component, the contributions of all variables sum to 1. The larger a contribution of a *observation/variable/table* to a component the more this *observation/variable/table* contributes to this component. 
Table contributions and partial inertias can be used to create plots that show the importance of these tables for the components. These plots can be drawn one component at a time or two (or rarely three) components at a time in a manner analogous to factor maps.

### 2.8 $R_v$ Coefficient and $L_g$ Coefficient{#anchor_co}

To evaluate the similarity between two tables one can compute coefficients of similarity between data tables. There are two kind of coefficients in our $MFA$ package: $R_v$ coefficient reflecting the amount of variance shared by two matrices and $L_g$ coefficient reflecting the MFA normalization and taking positive values.  Specifically, $R_V$  coefficient between data tables $k$ and $k′$ is computed as

$$\begin{equation}
R_{Vk,k'}=\frac{\text{trace}\{(\textbf{X}_{[k]}\textbf{X}_{[k]}^T)× (\textbf{X}_{[k']}\textbf{X}_{[k']}^T)\}}{\sqrt{\text{trace}\{(\textbf{X}_{[k]}\textbf{X}_{[k]}^T)× (\textbf{X}_{[k]}\textbf{X}_{[k]}^T)\}× \text{trace}\{(\textbf{X}_{[k']}\textbf{X}_{[k']}^T)× (\textbf{X}_{[k']}\textbf{X}_{[k']}^T)\}}}
\end{equation}$$
$L_g$  coefficient between data tables $k$ and $k′$ is computed as
$$\begin{equation}
\begin{aligned}
L_{g(k,k')}&=\frac{\text{trace}\{(\textbf{X}_{[k]}\textbf{X}_{[k]}^T)× (\textbf{X}_{[k']}\textbf{X}_{[k']}^T)\}}{\gamma_{1,k}^2 × \gamma_{1,k'}^2}\\
&= \text{trace}\{(\textbf{X}_{[k]}\textbf{X}_{[k]}^T)× (\textbf{X}_{[k']}\textbf{X}_{[k']}^T)\}× \alpha_k × \alpha_{k'}
\end{aligned}
\end{equation}$$


### 2.9 Bootstrap for Factor Scores{#anchor_bs}

The bootstrap is using to estimate the stability of the compromise factor scores. The main idea is to
use the properties that the compromise factor scores are the average of the partial factor scores. Therefore, we can obtain bootstrap confidence intervals (CIs) by repeatedly sampling with replacement from the set of tables and compute new compromise factor scores. From these estimates we can also compute **bootstrap ratios** for each dimension by dividing the mean of the bootstrap estimates by their standard deviation.  
To compute a bootstrap estimate, first we need to generate a bootstrap sample. To do so, we take a sample of integers with replacement from the set of integers from 1 to $K$. We call this set $\mathbb{B}$ (for bootstrap). Then generate a new data set (i.e., a new $\textbf{X}$ matrix comprising $K$ tables) using matrices $\textbf{X}_{[k]}$ with these indices. Combine all these sample blocks in a data matrix denoted $\textbf{X}_1^*$ that would then be analyzed by $MFA$. This would provide a set of bootstrapped factor scores (denoted $\textbf{F}_1^*$). Then repeat the procedure a large number of times (e.g., $L$ = 1000) and generate $L$ bootstrapped matrices of factor scores $\textbf{F}_l^*$ . $\bar{\textbf{F}}^∗$ denotes the bootstrap estimated factor scores, and is computed as
$$\begin{equation}
\bar{\textbf{F}}^∗=\frac{1}{L}\sum_l^L \textbf{F}_l^*
\end{equation}$$
$\hat\sigma_{F^*}^2$ denotes the bootstrapped estimate of the variance and is computed as
$$\begin{equation}
\hat\sigma_{F^*}^2=\frac{1}{L}\big(\sum_l^L(\textbf{F}_l^*-\bar{\textbf{F}}^∗) ◦ (\textbf{F}_l^*-\bar{\textbf{F}}^∗)\big)
\end{equation}$$
**Bootstrap ratios** is computed as
$$\begin{equation}
\frac{\bar{\textbf{F}}^∗}{\hat\sigma_{F^*}}
\end{equation}$$


## 3 Step by step tutorial

We are using a data example included in the package MFA to illustrate the use of the MFA package step by step. This data example concerns 12 wines from three
wine regions and 10 expert assessors
were asked to evaluate these wines on 9-point rating
scales, using four standard variables plus extra variables if any aseessor feels necessary. We call the variables used by one assessor as one group or one table. 

### 3.1 Getting started

To run any of the MFA functions, it is necessary to make the package active by using the library command:
```{r}
library(MFA)
```

#### Prepare the input data

The data on which we apply MFA functions should be either data frame or matrix object in R. The users could of course read data from a local file and here are of course many ways to enter data into R, just make sure prepare the data to be analyzed as data frame or matrix. The data example 'wine' has been loaded in the global environment in R when loading the MFA package, we can use head( ) to check the 'wine' data. As the data example is of many variables, we will choose the first 12 to show:
```{r }
head(wine)[,1:12]
```

It is required that the data on which we apply MFA functions is organized in certain way that the functions could separate each group by the arrangement of the columns of the input data, as shown in the data example 'wine', the variables of each group are stacked together and one group after another, like the first 6 columns are in Group 1 and the next 6 columns are in Group2, and so on:
```{r}
colnames(wine)
```
It's also recommended that the input data has row names and columns names for more readable outputs of the MFA functions. The reasons will be elaborated in the next sections.

### 3.2 mfa

We can now begin to use `mfa()` function to build a MFA model on data example 'wine'. The usage of `mfa()` is:
```{r,eval=FALSE}
mfa(data,sets,ncomps=NULL,center=TRUE,scale=TRUE)
```

#### data
`data` is a dataframe or matrix of numerical values

#### sets
`sets` can be specified with: 

*   A list of numbers indicating column indicies of each data groups: e.g., for this wine data set if we only want to analyze the first two groups, we specify `sets=list(1:6,7:12)` means columns 1 to 6 are variables of wine accessor no.1, columns 7 to 12 are variables of wine accessor no.2.

*   A list of variable names indicating column names of each data groups,you can use the first and last variable names of each group: e.g., for this wine data sets, we can use `sets=list(c("V1.G1","V6.G1"),c("V1.G2","V8.G2"))` for data group 1 and data group 2. Alternatively, you can use the full list of variable names of each group: e.g., `sets=list(c("V1.G1","V2.G1","V3.G1","V4.G1","V5.G1","V6.G1"),
c("V1.G2","V2.G2","V3.G2","V4.G2","V7.G2","V8.G2"))` for data group 1 and data group 2. 

#### ncomp
`ncomps` controls the number of components to be considered by the model. It can be specified with:

*   The default value `ncomps=NULL`: the `mfa()` will simply output results of all components. 

*   An integer smaller than the rank of the input data: e.g., for this wine data set we can spesify `ncomps=3`, which limit all the calculations in `mfa()` to the firs 3 components of the data.

#### center
`center` determines how column centering of the input data is performed. It can be specified with:

*   A logical value: the default value `center=TRUE` meaning centering is done by subtracting the column means (omitting NAs) of x from their corresponding columns. If center is FALSE, no centering is done.
*   A numeric vector of length equal to the number of columns of the input data: centerting is done by each column subtracting the corresponding value from the `center` vector. 

#### scale
`scale` determines how column scaling of the input data is performed AFTER centering. It can be specified with:

*   A logical value: the default value `scale=TRUE` means scaling is done by dividing the (centered) columns of x by their standard deviations if center is TRUE, and the root mean square otherwise. If scale is FALSE, no scaling is done.
*   A numeric vector of length equal to the number of columns of the input data: scaling is done by each column divided by the corresponding value from scale. 

Now we use the `mfa()` on the 'wine' data set, choose to output the first 7 components and center and scale the data, then store the model returned in 'mfa_wine':
```{r,results='hide',warning=F,messages=F}
varlist<-list(1:6,7:12,13:18,19:23,24:29,30:34,35:38,39:44,45:49,50:53)
mfa_wine<-mfa(wine,sets=varlist,ncomps=7,center=TRUE,scale=TRUE)
```
Use `slotNames(mfa_wine)` we can see elements of 'mfa_wine' model we just created:
```{r,results='hide',warnings=F,messages=F}
slotNames(mfa_wine)
```
And use `@` operatior to extract elements of the 'mfa_wine' model
```{r,warnings=F,messages=F}
mfa_wine@eigenvalues
```
```{r,warnings=F,messages=F}
mfa_wine@common_factor_score
```
```{r,warnings=F,messages=F}
mfa_wine@partial_factor_score[1]
```
```{r,warnings=F,messages=F}
head(mfa_wine@loadings)
```

### 3.3 print

Use `print()` on an object returned by `mfa()`, e.g. the 'mfa_wine' in this tutorial, will display the very basic information of the object.

```{r}
print(mfa_wine)
```

### 3.4 plot

Use `plot()` on an object returned by `mfa()`, e.g. the 'mfa_wine' in this tutorial, will display graphs with regard to the object and also save the image in high resolution in the working directory named 'mfa.jpeg'. The usage of `plot()` is:
```{r,eval=FALSE}
plot(x,dim,singleoutput=NULL)
```

#### `x`
`x` must be an object return by `mfa()`.

#### `dim`
`dim` determines which two dimensions the graphs will be based on. It can be specified with:

*   A vector of integers: e.g., `dim=c(1,2)` means plot the graphs on dimension 1 and dimension 2.

#### `singleoutput`
`singleoutput` determines whether to display all plots in the same graphs or display one specific single graph. It can be specified with:

*   The default value `singleoutput=NULL`: the `plot()` will display plots of eigenvalues, compromise factor scores, partial factor scores, and loadings(rescaled to have singular values ad variances) on the same graph. This graph will be saved as 'mfa.jpeg' in the working directory.

*   A charater string indicating which graph to be plotted: e.g. `singleoutput='eig'` will display a barchar of eigenvalues, `singleoutput='com'` will display a scatter plot of compromise factor scores, and `singleoutput='par'` will display scatter plots of each group's partial factor scores and loadings(rescaled to have singular values ad variances). This graph will be saved as 'mfa.jpeg' in the working directory.

```{r, out.width = '720px', out.length = '700px',dpi=500,fig.asp=0.8,fig.width=9,fig.length=5,fig.align='center',message=F,results='hide'}
plot(mfa_wine,dim=c(1,2),singleoutput=NULL)
```
```{r, out.width = '500px', out.length = '500px',dpi=500,fig.width=6,fig.asp=0.8,fig.align='center',message=F,results='hide'}
plot(mfa_wine,dim=c(1,2),singleoutput='eig')
```
```{r, out.width = '500px', out.length = '500px',dpi=500,fig.width=6,fig.asp=0.8,fig.align='center',message=FALSE,results='hide'}
plot(mfa_wine,dim=c(1,2),singleoutput='com')
```
```{r, out.width = '720px', out.length = '600px',dpi=2000,fig.width=9,fig.asp=0.5,message=FALSE,fig.align='right',results='hide'}
plot(mfa_wine,dim=c(1,2),singleoutput='par')
```

### 3.5 eigenvalues

`eigenvalues()` is a function that takes 'mfa' object and returns a table with the singular values(i.e. square root of eigenvalues), the eigenvalues, cumulative, percentage of intertia and cumulative percentage of inertia for all the extracted components. To see detailed explanation of the eigenvalues, see [2.5 Obtain eiganvalues and loadings](#anchor)
```{r}
eigenvalues(mfa_wine)
```

### 3.6 contributions

`contributions()` is a function that takes 'mfa' object and returns a list of matrix with 1). Contribution of an observation to a dimension. 2). Contribution of a variable to a dimension. 3). Contribution of a table to a dimension. These values help interpreting how observations,variables and tables contribute to the variability of the extracted dimensions. To see detailed explanations of eac contribution, see [2.7.1 Contributions of Observation](#anchor1),  [2.7.2 Contributions of Variables](#anchor2), and [2.7.3 Contributions of Table](#anchor3).

We can use `str()` to see the structure of the list return by `contributions()`. 
```{r}
str(contributions(mfa_wine))
```
Obviously the list has 3 matrices of the names 'observations','variables' and 'table', each can accessed by the operator `$`:
```{r}
contributions(mfa_wine)$observations
```
```{r}
contributions(mfa_wine)$table
```
As there are quite a few variables, we use `head()` to display the first several lines: 
```{r}
head(contributions(mfa_wine)$variables)
```

### 3.7 RV and LG

`RV()` takes two groups of data and return the $R_v$ Coefficient between them. Likewise, `LG()`takes two groups of data and return the $L_g$ Coefficient between them. To see detailed explanation of these two types of coeffecients, see [2.8 $R_v$ Coefficient and $L_g$ Coefficient](#anchor_co).
The usage of `RV()` is:
```{r,eval=FALSE}
RV(table1,table2)
```
`talble1` and `table2` are two groups of normalized data tables each contains their own variables. e.g. for 'wine' data set, the first 6 columns belongs to Group 1 and the next 6 columns belongs to Group2, we first normalized the two tables use R's `scale()` function and then use `RV()` to compute their $R_v$ Coefficient:
```{r}
table1<-scale(wine[,1:6])
table2<-scale(wine[,7:12])
RV(table1,table2)
```
Similarly, `LG()` takes two normalized data tables and for the first two groups of 'wine' data set we use `LG()` to compute their $L_g$ Coefficient:
```{r}
table1<-scale(wine[,1:6])
table2<-scale(wine[,7:12])
LG(table1,table2)
```

### 3.8 RV_table and LG_table

`RV_table()` takes a data set and a list with sets of variables, and return a symmetric matrix of $R_v$ Coefficients, and a symmetric matrix of $L_g$ coefficients for `LG_table()`. To see detailed explanation, see [2.8 $R_v$ Coefficient and $L_g$ Coefficient](#anchor_co).
The usage of `RV_table()` is:
```{r,eval=FALSE}
RV_table(dataset,sets)
```
`dataset` is a normalized data set and `sets` is a list like the `sets` in `mfa()` function indicating the variables of each group. E.g. for the 'wine' data set, to compute the $R_v$ Coefficients between each pair of the first 3 data groups, we can use `RV_table()`:
```{r}
nor_wine<-scale(wine[,1:18])
RV_table(nor_wine,sets=list(1:6,7:12,13:18))
```
Similarly for `LG_table()`:
```{r}
nor_wine<-scale(wine[,1:18])
LG_table(nor_wine,sets=list(1:6,7:12,13:18))
```

### 3.9 bootstrap
MFA is a descriptive multivariate technique, but it is often important to be able to complement the descriptive conclusions of an analysis by assessing if its results are reliable and replicable. `bootstrap()` is a function that takes the output of `mfa()` and return the boostrap ratio (by dividing each bootstrap mean by its standard deviation) of the compromise factor scores. To see the detailed explanation of how `bootstrap()`estimates the stability of the compromise factor scores, see [2.9 Bootstrap for Factor Scores](#anchor_bs). The useage of `bootstrap()` is:
```{r,eval=FALSE}
bootstrap(object,nbt=1000)
```
* `object` is an object as an output of `mfa()` function like 'mfa_Wine'
* `nbt` is an integer value stands for the number of bootstrap samples with a default value of 1000.
Use `bootstrap()` on our 'mfa_wine' to get the bootstrap ratios:
```{r}
bootstrap(mfa_wine,nbt=10000)
```

# 4 Shiny App

We have created a Shiny App for visualization of the output of this package. Please see our Shiny App at [MFA Shiny App](https://zyz2012.shinyapps.io/ShinyApp/)In this part, we will give a brief introduction about how to use the shiny app to do multiple factor analysis. Users are required to give the inputs including **data**,**set**, and the **dimesions** of the outcomes which they want to plot. 

## Input
`data`: Users should choose data of **.csv** format with the header and with the first column as the id. If nothing is chosen, the shiny app will plot the outcome of the *wine.csv* as default.

`sets`: Users should choose sets of **.txt** format. The .txt file should have the sets of the format below:
$$1:6,7:12,13:18,19:23,24:29,30:34,35:38,39:44,45:53$$
(Use "**,**" to seperate different groups, and "**:**" to separate the starting and ending columns in the data set)
If *sets* is not given, the shiny app will plot the outcome of the *sets.txt* as an example.

`dim`: Usually, **dim** equals **c(1,2)** to assign the most important two components.

##Output
`A table` shows the outcome of eigenvalues, partial factor scores, common factor scores and loadings respectively.
`Four pictures` show the outcome of eigenvalues, partial factor scores, common factor scores and loadings.

**Notes**
1.When plotting the outcome of partial factor scores, it may take some time due to the large size of the picture.

